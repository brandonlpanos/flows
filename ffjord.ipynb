{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d79ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e043f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base distribution (a 1D Gaussian)\n",
    "base_dist = dist.normal.Normal(loc=torch.tensor([0.]), scale=torch.tensor([1.]))\n",
    "\n",
    "# Define the target distribution (a bimodal distribution)\n",
    "target_dist = dist.normal.Normal(loc=torch.tensor([-2., 2.]), scale=torch.tensor([0.5, 0.5]))\n",
    "\n",
    "# Define the dynamics function\n",
    "class Dynamics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers of the dynamics function\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x, t=None):\n",
    "        \"\"\"\n",
    "        This function computes the dynamics of the flow model.\n",
    "\n",
    "        Args:\n",
    "            x: the input tensor\n",
    "            t: the current time (not used in this example)\n",
    "\n",
    "        Returns:\n",
    "            The output tensor of the dynamics function and the log determinant of the Jacobian matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply the layers of the dynamics function\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        h = self.fc3(h)\n",
    "\n",
    "        # Compute the scale parameter using the output of the final layer and the softplus activation function\n",
    "        scale = self.softplus(h)\n",
    "\n",
    "        # Compute the output of the dynamics function and the log determinant of the Jacobian matrix\n",
    "        return scale * x, torch.sum(torch.log(scale), dim=-1)\n",
    "\n",
    "# Define the continuous flow model\n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, dynamics):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the dynamics function as a module attribute\n",
    "        self.dynamics = dynamics\n",
    "\n",
    "    def forward(self, x, log_jac0=None, reverse=False):\n",
    "        \"\"\"\n",
    "        This function computes the forward or inverse transformation of the flow model.\n",
    "\n",
    "        Args:\n",
    "            x: the input tensor\n",
    "            log_jac0: the initial value of the log determinant of the Jacobian matrix (not used in this example)\n",
    "            reverse: if True, compute the inverse transformation\n",
    "\n",
    "        Returns:\n",
    "            The transformed tensor and the log determinant of the Jacobian matrix\n",
    "        \"\"\"\n",
    "\n",
    "        if not reverse:\n",
    "            # If computing the forward transformation, apply the dynamics function to the input tensor\n",
    "            return self.dynamics(x, t=0.)\n",
    "        else:\n",
    "            # If computing the inverse transformation, apply the dynamics function to the input tensor in reverse order\n",
    "            z = x\n",
    "            log_jac = log_jac0\n",
    "            for i in range(len(self.dynamics)):\n",
    "                z, log_det = self.dynamics[-1-i](z, t=0.)\n",
    "                log_jac = log_jac - log_det\n",
    "            return z, log_jac\n",
    "\n",
    "# Define the loss function (negative log-likelihood)\n",
    "def loss_fn(x, log_jac):\n",
    "    \"\"\"\n",
    "    This function computes the negative log-likelihood of the transformed distribution.\n",
    "\n",
    "    Args:\n",
    "        x: the transformed tensor\n",
    "        log_jac: the log determinant of the Jacobian matrix\n",
    "\n",
    "    Returns:\n",
    "        The negative log-likelihood of the transformed distribution\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the negative log-likelihood of the target distribution using the log probability function of PyTorch's distribution module\n",
    "    return -target_dist.log_prob(x).sum() - log_jac.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4dd0f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dynamics \u001b[38;5;241m=\u001b[39m Dynamics()\n\u001b[1;32m      3\u001b[0m flow \u001b[38;5;241m=\u001b[39m Flow([dynamics])\n\u001b[0;32m----> 4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the flow model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/smlbook/env_sml_book/lib/python3.9/site-packages/torch/optim/sgd.py:109\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov \u001b[38;5;129;01mand\u001b[39;00m (momentum \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dampening \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/smlbook/env_sml_book/lib/python3.9/site-packages/torch/optim/optimizer.py:61\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     59\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     63\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "# Define the optimizer (stochastic gradient descent)\n",
    "dynamics = Dynamics()\n",
    "flow = Flow([dynamics])\n",
    "optimizer = optim.SGD(flow.parameters(), lr=0.001)\n",
    "\n",
    "# Train the flow model\n",
    "num_epochs = 10000\n",
    "batch_size = 256\n",
    "for epoch in range(num_epochs):\n",
    "    # Generate a batch of samples from the base distribution\n",
    "    batch_x = base_dist.sample((batch_size,))\n",
    "\n",
    "    # Transform the samples using the flow model\n",
    "    y, log_jac = flow(batch_x)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(y, log_jac)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss every 10 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "# Evaluate the trained model\n",
    "# Generate a large number of samples from the transformed distribution\n",
    "num_samples = 10000\n",
    "samples = []\n",
    "for _ in range(num_samples // batch_size):\n",
    "    # Generate a batch of samples from the base distribution\n",
    "    batch_x = base_dist.sample((batch_size,))\n",
    "\n",
    "    # Transform the samples using the flow model\n",
    "    y, log_jac = flow(batch_x)\n",
    "    samples.append(y.detach().numpy())\n",
    "\n",
    "samples = np.concatenate(samples)\n",
    "\n",
    "# Compute the empirical mean and standard deviation of the transformed samples\n",
    "mean = np.mean(samples)\n",
    "std = np.std(samples)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Transformed distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5cc27a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ffjord.model'; 'ffjord' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mffjord\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FFJORD\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define a simple 1D Gaussian as the base distribution\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGaussianDistribution\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ffjord.model'; 'ffjord' is not a package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from ffjord.model import FFJORD\n",
    "\n",
    "\n",
    "# Define a simple 1D Gaussian as the base distribution\n",
    "class GaussianDistribution(nn.Module):\n",
    "    def __init__(self, mu, sigma):\n",
    "        super(GaussianDistribution, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (-((x - self.mu)**2) / (2 * self.sigma**2)).exp() / self.sigma\n",
    "\n",
    "\n",
    "# Define a bimodal distribution as the target distribution\n",
    "class BimodalDistribution(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.exp(-0.5*((x-2)**2)/0.5) + torch.exp(-0.5*((x+2)**2)/0.5)\n",
    "\n",
    "\n",
    "# Define the time dynamic\n",
    "class TimeDynamics(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(TimeDynamics, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        # Define the time-dependent coefficient function\n",
    "        # In this example, we use a simple linear function\n",
    "        a_t = torch.tensor([[1.0 + 0.5 * t]])\n",
    "\n",
    "        # Multiply the coefficient function with the noise input\n",
    "        a_t = a_t.expand(z.shape[0], -1)\n",
    "\n",
    "        # Return the time derivative of z\n",
    "        return -torch.mm(a_t, z.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\n",
    "# Define the flow model using FFJORD\n",
    "class FlowModel(nn.Module):\n",
    "    def __init__(self, dim, time_dynamics):\n",
    "        super(FlowModel, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        # Define the FFJORD module with the Gaussian distribution and time dynamics\n",
    "        self.ffjord = FFJORD(\n",
    "            dim=self.dim,\n",
    "            time_dynamics=time_dynamics,\n",
    "            train_T=True,\n",
    "            solver='dopri5'\n",
    "        )\n",
    "\n",
    "        # Define the base distribution as a 1D Gaussian with mean=0 and standard deviation=1\n",
    "        self.base_dist = GaussianDistribution(mu=torch.tensor([0.0]), sigma=torch.tensor([1.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate samples from the base distribution\n",
    "        z, log_jac = self.ffjord(x)\n",
    "\n",
    "        # Compute the log probability density of the target distribution using the change of variable formula\n",
    "        # log p(x) = log p(z) + log det |dx/dz|\n",
    "        # We use the log_sum_exp trick to improve numerical stability\n",
    "        log_p = torch.logsumexp(torch.log(self.base_dist(z)) + log_jac, dim=1)\n",
    "\n",
    "        # Return the negative log probability as the loss\n",
    "        return -log_p.mean()\n",
    "\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, optimizer, dataloader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        loss_total = 0.0\n",
    "        for x, _ in dataloader:\n",
    "            # Reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss = model(x)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_total += loss.item()\n",
    "\n",
    "        # Print the average loss over the dataset for each epoch\n",
    "        print(\"Epoch {}/{}: Loss = {:.4f}\".format(epoch + 1, epochs, loss_total / len(dataloader)))\n",
    "\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3e74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "class Dynamics(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Dynamics, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.fc3(h)\n",
    "        return h\n",
    "\n",
    "class ContinuousNF(nn.Module):\n",
    "    def __init__(self, hidden_size, num_blocks, solver):\n",
    "        super(ContinuousNF, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.dynamics = nn.ModuleList([Dynamics(hidden_size) for _ in range(num_blocks)])\n",
    "        self.solver = solver\n",
    "\n",
    "    def forward(self, z0):\n",
    "        zs = [z0]\n",
    "        log_det_Js = []\n",
    "        for i in range(self.num_blocks):\n",
    "            t0 = torch.zeros(z0.size(0)).to(z0.device)\n",
    "            z1, log_det_J = self.integrate(self.dynamics[i], zs[-1], t0, t0 + 1.0)\n",
    "            zs.append(z1)\n",
    "            log_det_Js.append(log_det_J)\n",
    "        return torch.cat(zs[1:], dim=0), sum(log_det_Js)\n",
    "\n",
    "    def integrate(self, f, z0, t0, t1):\n",
    "        def grad(z, t):\n",
    "            z = torch.autograd.Variable(z, requires_grad=True)\n",
    "            dzdt = f(t, z)\n",
    "            d2zdt2 = torch.autograd.grad(dzdt.sum(), z, create_graph=True)[0]\n",
    "            return dzdt, d2zdt2\n",
    "\n",
    "        zs = odeint(grad, z0, torch.tensor([t0, t1]).float(), method=self.solver)\n",
    "        z1, zlog_det_J = zs[-1], 0.0\n",
    "        for i in range(len(zs) - 1):\n",
    "            dzdt, d2zdt2 = grad(zs[i], t0 + i * (t1 - t0) / (len(zs) - 1))\n",
    "            zlog_det_J += torch.log(torch.abs(torch.det(torch.eye(z0.size(-1)).to(z0.device) + (t1 - t0) / (len(zs) - 1) * dzdt + ((t1 - t0) / (len(zs) - 1))**2 * d2zdt2)))\n",
    "        return z1, zlog_det_J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44da5e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compute the transformed samples and log determinant of the Jacobian\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m x, log_det \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compute the negative log-likelihood\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(base_dist\u001b[38;5;241m.\u001b[39mlog_prob(z) \u001b[38;5;241m+\u001b[39m log_det)\n",
      "File \u001b[0;32m~/Desktop/smlbook/env_sml_book/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mContinuousNF.forward\u001b[0;34m(self, z0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_blocks):\n\u001b[1;32m     31\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(z0\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(z0\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 32\u001b[0m     z1, log_det_J \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     zs\u001b[38;5;241m.\u001b[39mappend(z1)\n\u001b[1;32m     34\u001b[0m     log_det_Js\u001b[38;5;241m.\u001b[39mappend(log_det_J)\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mContinuousNF.integrate\u001b[0;34m(self, f, z0, t0, t1)\u001b[0m\n\u001b[1;32m     41\u001b[0m     d2zdt2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(dzdt\u001b[38;5;241m.\u001b[39msum(), z, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dzdt, d2zdt2\n\u001b[0;32m---> 44\u001b[0m zs \u001b[38;5;241m=\u001b[39m odeint(grad, z0, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat(), method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver)\n\u001b[1;32m     45\u001b[0m z1, zlog_det_J \u001b[38;5;241m=\u001b[39m zs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(zs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = ContinuousNF(5, 3, odeint)\n",
    "batch_size = 64\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Sample a batch of data from the base distribution\n",
    "    z = torch.randn(batch_size, 1)\n",
    "\n",
    "    # Compute the transformed samples and log determinant of the Jacobian\n",
    "    x, log_det = model(z)\n",
    "\n",
    "    # Compute the negative log-likelihood\n",
    "    loss = -torch.mean(base_dist.log_prob(z) + log_det)\n",
    "\n",
    "    # Backpropagate and update parameters\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c4a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ffjord (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for ffjord\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ffjord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "962ecf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: -0.03594920039176941\n",
      "Epoch [101/1000], Loss: -0.18914613127708435\n",
      "Epoch [201/1000], Loss: -0.19933584332466125\n",
      "Epoch [301/1000], Loss: -0.1994125097990036\n",
      "Epoch [401/1000], Loss: -0.19942568242549896\n",
      "Epoch [501/1000], Loss: -0.19943687319755554\n",
      "Epoch [601/1000], Loss: -0.19945496320724487\n",
      "Epoch [701/1000], Loss: -0.19945508241653442\n",
      "Epoch [801/1000], Loss: -0.19946414232254028\n",
      "Epoch [901/1000], Loss: -0.1994565725326538\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "# Define the base distribution (1D Gaussian)\n",
    "def gaussian(x, mu=0, sigma=1):\n",
    "    return torch.exp(-0.5 * ((x - mu) / sigma) ** 2) / (sigma * torch.sqrt(2 * torch.tensor(3.141592653589793)))\n",
    "\n",
    "# Define the target distribution (bimodal)\n",
    "def bimodal(x):\n",
    "    return 0.5 * gaussian(x, mu=-2, sigma=0.5) + 0.5 * gaussian(x, mu=2, sigma=1)\n",
    "\n",
    "# Define the flow model using a neural ODE\n",
    "class FlowModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlowModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.t = torch.linspace(0, 1, 10)\n",
    "        \n",
    "    def forward(self, t, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        h = self.fc2(h)\n",
    "        return h, torch.zeros_like(h)\n",
    "    \n",
    "    def backward(self, z):\n",
    "        with torch.no_grad():\n",
    "            x = odeint(self, z, self.t, method='dopri5', rtol=1e-4, atol=1e-5)[-1]\n",
    "        return x\n",
    "    \n",
    "# Define the loss function\n",
    "def loss_fn(z, log_jacobian):\n",
    "    return -torch.mean(bimodal(z) + log_jacobian)\n",
    "\n",
    "# Define the training loop\n",
    "def train_flow_model(flow_model, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        z = torch.randn(128, 1)\n",
    "        x, log_jacobian = flow_model(0, z) # pass in time t=0\n",
    "        loss = loss_fn(x, log_jacobian)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# Instantiate the flow model and optimizer\n",
    "flow_model = FlowModel()\n",
    "optimizer = torch.optim.Adam(flow_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the flow model\n",
    "train_flow_model(flow_model, optimizer, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c554dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6344, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "# Define the continuous-time dynamics using an ordinary differential equation (ODE)\n",
    "class Dynamics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        # t is time (not used in this case)\n",
    "        # x is the input\n",
    "        dxdt = -self.net(x.pow(3))\n",
    "        return dxdt\n",
    "\n",
    "# Define the continuous normalizing flow model\n",
    "class CNF(nn.Module):\n",
    "    def __init__(self, dynamics):\n",
    "        super().__init__()\n",
    "        self.dynamics = dynamics\n",
    "\n",
    "    def forward(self, x, logpx=None):\n",
    "        # Reshape x to be a column vector\n",
    "        x = x.view(-1, 1)\n",
    "\n",
    "        # Compute the derivative of x with respect to t using the ODE solver\n",
    "        # We use the adjoint method for backpropagation through the ODE solver\n",
    "        t = torch.Tensor([0, 1]).to(x)\n",
    "        xt = odeint(self.dynamics, x, t, rtol=1e-5, atol=1e-6)\n",
    "\n",
    "        # Reshape xt to be a row vector\n",
    "        xt = xt.view(1, -1)\n",
    "\n",
    "        # Compute the log determinant of the Jacobian matrix\n",
    "        # This is needed for computing the likelihood of the transformed distribution\n",
    "        logdet = torch.zeros(x.shape[0], device=x.device)\n",
    "\n",
    "        if logpx is not None:\n",
    "            logpx = logpx - logdet\n",
    "\n",
    "        return xt, logpx\n",
    "\n",
    "# Define the simple 1D Gaussian distribution\n",
    "class Gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def sample(self, n):\n",
    "        return torch.randn(n) * self.sigma + self.mu\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return -0.5 * ((x - self.mu) / self.sigma) ** 2 - torch.log(torch.sqrt(2 * torch.tensor([3.1415])) * self.sigma)\n",
    "\n",
    "# Instantiate the continuous normalizing flow model and the simple 1D Gaussian distribution\n",
    "dynamics = Dynamics()\n",
    "cnf = CNF(dynamics)\n",
    "gaussian = Gaussian(mu=0.5, sigma=0.1)\n",
    "\n",
    "# Sample from the simple 1D Gaussian distribution\n",
    "x = gaussian.sample(100)\n",
    "\n",
    "# Transform the samples using the continuous normalizing flow model\n",
    "xt, logpx = cnf(x)\n",
    "\n",
    "# Compute the likelihood of the transformed samples under the transformed distribution\n",
    "log_prob = gaussian.log_prob(xt.view(-1))\n",
    "\n",
    "# Print the average log likelihood\n",
    "print(log_prob.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea7e26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "class NeuralODE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(1, 32)\n",
    "        self.linear2 = torch.nn.Linear(32, 1)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, t, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "    \n",
    "def bimodal_target(x):\n",
    "    return torch.exp(-(x-2)**2) + torch.exp(-(x+2)**2)\n",
    "    \n",
    "def train_neural_ode(num_epochs=5000):\n",
    "    # Define the initial condition\n",
    "    x0 = torch.tensor([0.0], requires_grad=True)\n",
    "    t = torch.linspace(0.0, 1.0, 100)\n",
    "    \n",
    "    # Define the neural ODE\n",
    "    net = NeuralODE()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Train the neural ODE\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        pred = odeint(net, x0, t)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = torch.mean((pred[:, 0] - bimodal_target(t))**2)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print the loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch [{}/{}], Loss: {:.4f}\".format(epoch+1, num_epochs, loss.item()))\n",
    "    \n",
    "    # Return the final prediction\n",
    "    return odeint(net, x0, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "637dcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neural_ode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040009e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sml_book",
   "language": "python",
   "name": "env_sml_book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
