{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517bf86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b036383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base distribution (a 1D Gaussian)\n",
    "base_dist = dist.normal.Normal(loc=torch.tensor([0.]), scale=torch.tensor([1.]))\n",
    "\n",
    "# Define the target distribution (a bimodal distribution)\n",
    "target_dist = dist.normal.Normal(loc=torch.tensor([-2., 2.]), scale=torch.tensor([0.5, 0.5]))\n",
    "\n",
    "# Define the dynamics function\n",
    "class Dynamics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers of the dynamics function\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x, t=None):\n",
    "        \"\"\"\n",
    "        This function computes the dynamics of the flow model.\n",
    "\n",
    "        Args:\n",
    "            x: the input tensor\n",
    "            t: the current time (not used in this example)\n",
    "\n",
    "        Returns:\n",
    "            The output tensor of the dynamics function and the log determinant of the Jacobian matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply the layers of the dynamics function\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        h = self.fc3(h)\n",
    "\n",
    "        # Compute the scale parameter using the output of the final layer and the softplus activation function\n",
    "        scale = self.softplus(h)\n",
    "\n",
    "        # Compute the output of the dynamics function and the log determinant of the Jacobian matrix\n",
    "        return scale * x, torch.sum(torch.log(scale), dim=-1)\n",
    "\n",
    "# Define the continuous flow model\n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, dynamics):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the dynamics function as a module attribute\n",
    "        self.dynamics = dynamics\n",
    "\n",
    "    def forward(self, x, log_jac0=None, reverse=False):\n",
    "        \"\"\"\n",
    "        This function computes the forward or inverse transformation of the flow model.\n",
    "\n",
    "        Args:\n",
    "            x: the input tensor\n",
    "            log_jac0: the initial value of the log determinant of the Jacobian matrix (not used in this example)\n",
    "            reverse: if True, compute the inverse transformation\n",
    "\n",
    "        Returns:\n",
    "            The transformed tensor and the log determinant of the Jacobian matrix\n",
    "        \"\"\"\n",
    "\n",
    "        if not reverse:\n",
    "            # If computing the forward transformation, apply the dynamics function to the input tensor\n",
    "            return self.dynamics(x, t=0.)\n",
    "        else:\n",
    "            # If computing the inverse transformation, apply the dynamics function to the input tensor in reverse order\n",
    "            z = x\n",
    "            log_jac = log_jac0\n",
    "            for i in range(len(self.dynamics)):\n",
    "                z, log_det = self.dynamics[-1-i](z, t=0.)\n",
    "                log_jac = log_jac - log_det\n",
    "            return z, log_jac\n",
    "\n",
    "# Define the loss function (negative log-likelihood)\n",
    "def loss_fn(x, log_jac):\n",
    "    \"\"\"\n",
    "    This function computes the negative log-likelihood of the transformed distribution.\n",
    "\n",
    "    Args:\n",
    "        x: the transformed tensor\n",
    "        log_jac: the log determinant of the Jacobian matrix\n",
    "\n",
    "    Returns:\n",
    "        The negative log-likelihood of the transformed distribution\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the negative log-likelihood of the target distribution using the log probability function of PyTorch's distribution module\n",
    "    return -target_dist.log_prob(x).sum() - log_jac.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3796a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dynamics \u001b[38;5;241m=\u001b[39m Dynamics()\n\u001b[1;32m      3\u001b[0m flow \u001b[38;5;241m=\u001b[39m Flow([dynamics])\n\u001b[0;32m----> 4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the flow model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/smlbook/env_sml_book/lib/python3.9/site-packages/torch/optim/sgd.py:109\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov \u001b[38;5;129;01mand\u001b[39;00m (momentum \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dampening \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/smlbook/env_sml_book/lib/python3.9/site-packages/torch/optim/optimizer.py:61\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     59\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     63\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "# Define the optimizer (stochastic gradient descent)\n",
    "dynamics = Dynamics()\n",
    "flow = Flow([dynamics])\n",
    "optimizer = optim.SGD(flow.parameters(), lr=0.001)\n",
    "\n",
    "# Train the flow model\n",
    "num_epochs = 10000\n",
    "batch_size = 256\n",
    "for epoch in range(num_epochs):\n",
    "    # Generate a batch of samples from the base distribution\n",
    "    batch_x = base_dist.sample((batch_size,))\n",
    "\n",
    "    # Transform the samples using the flow model\n",
    "    y, log_jac = flow(batch_x)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(y, log_jac)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss every 10 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "# Evaluate the trained model\n",
    "# Generate a large number of samples from the transformed distribution\n",
    "num_samples = 10000\n",
    "samples = []\n",
    "for _ in range(num_samples // batch_size):\n",
    "    # Generate a batch of samples from the base distribution\n",
    "    batch_x = base_dist.sample((batch_size,))\n",
    "\n",
    "    # Transform the samples using the flow model\n",
    "    y, log_jac = flow(batch_x)\n",
    "    samples.append(y.detach().numpy())\n",
    "\n",
    "samples = np.concatenate(samples)\n",
    "\n",
    "# Compute the empirical mean and standard deviation of the transformed samples\n",
    "mean = np.mean(samples)\n",
    "std = np.std(samples)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Transformed distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ff932f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ffjord.model'; 'ffjord' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mffjord\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FFJORD\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define a simple 1D Gaussian as the base distribution\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGaussianDistribution\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ffjord.model'; 'ffjord' is not a package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from ffjord.model import FFJORD\n",
    "\n",
    "\n",
    "# Define a simple 1D Gaussian as the base distribution\n",
    "class GaussianDistribution(nn.Module):\n",
    "    def __init__(self, mu, sigma):\n",
    "        super(GaussianDistribution, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (-((x - self.mu)**2) / (2 * self.sigma**2)).exp() / self.sigma\n",
    "\n",
    "\n",
    "# Define a bimodal distribution as the target distribution\n",
    "class BimodalDistribution(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.exp(-0.5*((x-2)**2)/0.5) + torch.exp(-0.5*((x+2)**2)/0.5)\n",
    "\n",
    "\n",
    "# Define the time dynamic\n",
    "class TimeDynamics(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(TimeDynamics, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        # Define the time-dependent coefficient function\n",
    "        # In this example, we use a simple linear function\n",
    "        a_t = torch.tensor([[1.0 + 0.5 * t]])\n",
    "\n",
    "        # Multiply the coefficient function with the noise input\n",
    "        a_t = a_t.expand(z.shape[0], -1)\n",
    "\n",
    "        # Return the time derivative of z\n",
    "        return -torch.mm(a_t, z.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\n",
    "# Define the flow model using FFJORD\n",
    "class FlowModel(nn.Module):\n",
    "    def __init__(self, dim, time_dynamics):\n",
    "        super(FlowModel, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        # Define the FFJORD module with the Gaussian distribution and time dynamics\n",
    "        self.ffjord = FFJORD(\n",
    "            dim=self.dim,\n",
    "            time_dynamics=time_dynamics,\n",
    "            train_T=True,\n",
    "            solver='dopri5'\n",
    "        )\n",
    "\n",
    "        # Define the base distribution as a 1D Gaussian with mean=0 and standard deviation=1\n",
    "        self.base_dist = GaussianDistribution(mu=torch.tensor([0.0]), sigma=torch.tensor([1.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate samples from the base distribution\n",
    "        z, log_jac = self.ffjord(x)\n",
    "\n",
    "        # Compute the log probability density of the target distribution using the change of variable formula\n",
    "        # log p(x) = log p(z) + log det |dx/dz|\n",
    "        # We use the log_sum_exp trick to improve numerical stability\n",
    "        log_p = torch.logsumexp(torch.log(self.base_dist(z)) + log_jac, dim=1)\n",
    "\n",
    "        # Return the negative log probability as the loss\n",
    "        return -log_p.mean()\n",
    "\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, optimizer, dataloader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        loss_total = 0.0\n",
    "        for x, _ in dataloader:\n",
    "            # Reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss = model(x)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_total += loss.item()\n",
    "\n",
    "        # Print the average loss over the dataset for each epoch\n",
    "        print(\"Epoch {}/{}: Loss = {:.4f}\".format(epoch + 1, epochs, loss_total / len(dataloader)))\n",
    "\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fc5379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "class Dynamics(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Dynamics, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.fc3(h)\n",
    "        return h\n",
    "\n",
    "class ContinuousNF(nn.Module):\n",
    "    def __init__(self, hidden_size, num_blocks, solver):\n",
    "        super(ContinuousNF, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.dynamics = nn.ModuleList([Dynamics(hidden_size) for _ in range(num_blocks)])\n",
    "        self.solver = solver\n",
    "\n",
    "    def forward(self, z0):\n",
    "        zs = [z0]\n",
    "        log_det_Js = []\n",
    "        for i in range(self.num_blocks):\n",
    "            t0 = torch.zeros(z0.size(0)).to(z0.device)\n",
    "            z1, log_det_J = self.integrate(self.dynamics[i], zs[-1], t0, t0 + 1.0)\n",
    "            zs.append(z1)\n",
    "            log_det_Js.append(log_det_J)\n",
    "        return torch.cat(zs[1:], dim=0), sum(log_det_Js)\n",
    "\n",
    "    def integrate(self, f, z0, t0, t1):\n",
    "        def grad(z, t):\n",
    "            z = torch.autograd.Variable(z, requires_grad=True)\n",
    "            dzdt = f(t, z)\n",
    "            d2zdt2 = torch.autograd.grad(dzdt.sum(), z, create_graph=True)[0]\n",
    "            return dzdt, d2zdt2\n",
    "\n",
    "        zs = odeint(grad, z0, torch.tensor([t0, t1]).float(), method=self.solver)\n",
    "        z1, zlog_det_J = zs[-1], 0.0\n",
    "        for i in range(len(zs) - 1):\n",
    "            dzdt, d2zdt2 = grad(zs[i], t0 + i * (t1 - t0) / (len(zs) - 1))\n",
    "            zlog_det_J += torch.log(torch.abs(torch.det(torch.eye(z0.size(-1)).to(z0.device) + (t1 - t0) / (len(zs) - 1) * dzdt + ((t1 - t0) / (len(zs) - 1))**2 * d2zdt2)))\n",
    "        return z1, zlog_det_J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23589157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compute the transformed samples and log determinant of the Jacobian\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m x, log_det \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compute the negative log-likelihood\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(base_dist\u001b[38;5;241m.\u001b[39mlog_prob(z) \u001b[38;5;241m+\u001b[39m log_det)\n",
      "File \u001b[0;32m~/Desktop/smlbook/env_sml_book/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mContinuousNF.forward\u001b[0;34m(self, z0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_blocks):\n\u001b[1;32m     31\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(z0\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(z0\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 32\u001b[0m     z1, log_det_J \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     zs\u001b[38;5;241m.\u001b[39mappend(z1)\n\u001b[1;32m     34\u001b[0m     log_det_Js\u001b[38;5;241m.\u001b[39mappend(log_det_J)\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mContinuousNF.integrate\u001b[0;34m(self, f, z0, t0, t1)\u001b[0m\n\u001b[1;32m     41\u001b[0m     d2zdt2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(dzdt\u001b[38;5;241m.\u001b[39msum(), z, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dzdt, d2zdt2\n\u001b[0;32m---> 44\u001b[0m zs \u001b[38;5;241m=\u001b[39m odeint(grad, z0, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat(), method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver)\n\u001b[1;32m     45\u001b[0m z1, zlog_det_J \u001b[38;5;241m=\u001b[39m zs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(zs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = ContinuousNF(5, 3, odeint)\n",
    "batch_size = 64\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Sample a batch of data from the base distribution\n",
    "    z = torch.randn(batch_size, 1)\n",
    "\n",
    "    # Compute the transformed samples and log determinant of the Jacobian\n",
    "    x, log_det = model(z)\n",
    "\n",
    "    # Compute the negative log-likelihood\n",
    "    loss = -torch.mean(base_dist.log_prob(z) + log_det)\n",
    "\n",
    "    # Backpropagate and update parameters\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddab3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ffjord (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for ffjord\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ffjord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a71928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Define the target distribution\n",
    "def bimodal(x):\n",
    "    return 0.5 * norm.pdf(x, loc=-1, scale=0.3) + 0.5 * norm.pdf(x, loc=1, scale=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa57c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base distribution\n",
    "def base_distribution(n_samples):\n",
    "    return np.random.randn(n_samples, 1).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc85fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "# Define the flow model using a neural ODE\n",
    "class FlowModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlowModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.t = torch.linspace(0, 1, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        h = self.fc2(h)\n",
    "        return h\n",
    "    \n",
    "    def backward(self, z):\n",
    "        with torch.no_grad():\n",
    "            x = odeint(self, z, self.t, method='dopri5', rtol=1e-4, atol=1e-5)\n",
    "        return x[-1]\n",
    "    \n",
    "# Define the loss function\n",
    "def loss_fn(z, log_jacobian):\n",
    "    return -torch.mean(bimodal(z) + log_jacobian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "027d320f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Compute the log jacobian\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mt)):\n\u001b[0;32m---> 28\u001b[0m     x, log_jac \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     log_jacobian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m log_jac\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/smlbook/env_sml_book/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Initialize the flow model\n",
    "model = FlowModel()\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Set the number of epochs and batch size\n",
    "n_epochs = 5000\n",
    "batch_size = 512\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Train the model\n",
    "for i in range(n_epochs):\n",
    "    # Generate samples from the base distribution\n",
    "    z = torch.from_numpy(base_distribution(batch_size))\n",
    "    \n",
    "    # Forward pass\n",
    "    x = model.backward(z)\n",
    "    log_jacobian = torch.zeros(batch_size)\n",
    "    \n",
    "    # Compute the log jacobian\n",
    "    for j in range(len(model.t)):\n",
    "        x, log_jac = model(x)\n",
    "        log_jacobian += log_jac\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = loss_fn(z, log_jacobian)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Plot the results every 100 epochs\n",
    "    if i % 100 == 0:\n",
    "        # Plot the target distribution\n",
    "        x_plot = np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff9421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sml_book",
   "language": "python",
   "name": "env_sml_book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
